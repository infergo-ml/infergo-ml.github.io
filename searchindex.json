[{"content":"Prerequisites infergo benefits from modules introduced in Go 1.11, and relies on go/packages to import packages in a module-aware way, but will work with earlier versions of Go. Install Go. It is easier to build and use infergo with the make utility.\nInstallation There are two installation options:\nIf your Go project imports any of infergo packages and uses modules, infergo will be installed for you. deriv utility will be in $GOPATH/bin. As a side effect of providing examples with the main repository, the example binaries will also be installed in $GOPATH/bin. They are not needed there, you can remove them.\nAlternatively, you can clone the repository and build infergo from the cloned directory:\ngit clone https://bitbucket.org/dtolpin/infergo cd infergo make install This will install only deriv but not any example binaries. To build examples (and run each of them on the embedded self-check dataset), run\nmake examples Hello world The probabilistic \u0026ldquo;Hello world\u0026rdquo; example from the infergo repository shows a typical project layout and commands to build a Go program with an infergo model. Explore the example\u0026rsquo;s source code and Makefile. In a nutshell, one needs to:\nimplement the model in a separate package, differentiate the model\u0026rsquo;s package with deriv, import the package with the differentiated model (ending in \u0026quot;ad/\u0026quot;) in the file where inference is performed, build the project in the normal Go way. ","date":"1 January, 0001","id":0,"permalink":"/start/","summary":"\u003ch2 id=\"prerequisites\"\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003einfergo\u003c/code\u003e benefits from\n\u003ca href=\"https://github.com/golang/go/wiki/Modules\"\u003emodules\u003c/a\u003e introduced\nin Go 1.11, and relies on\n\u003ca href=\"https://godoc.org/golang.org/x/tools/go/packages\"\u003e\u003ccode\u003ego/packages\u003c/code\u003e\u003c/a\u003e\nto import packages in a module-aware way, but will work with\nearlier versions of Go.  \u003ca href=\"https://golang.org/doc/install\"\u003eInstall\nGo\u003c/a\u003e. It is easier to build and\nuse \u003ccode\u003einfergo\u003c/code\u003e with the \u003ccode\u003emake\u003c/code\u003e utility.\u003c/p\u003e\n\u003ch2 id=\"installation\"\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eThere are two installation options:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eIf your Go project imports any of \u003ca href=\"https://godoc.org/bitbucket.org/dtolpin/infergo\"\u003e\u003ccode\u003einfergo\u003c/code\u003e\npackages\u003c/a\u003e and\nuses modules, \u003ccode\u003einfergo\u003c/code\u003e will be installed for you. \u003ccode\u003ederiv\u003c/code\u003e\nutility will be in \u003ccode\u003e$GOPATH/bin\u003c/code\u003e. As a side effect of\nproviding examples with the main repository, the example\nbinaries will also be installed in \u003ccode\u003e$GOPATH/bin\u003c/code\u003e. They are\nnot needed there, you can remove them.\u003c/p\u003e","tags":"","title":"Getting started"},{"content":"\nCode Model A model is defined in it\u0026rsquo;s own package. The model must implement interface model.Model, consisting of a single method Observe([]float64) float64. In the model\u0026rsquo;s source code:\nMethods on the type implementing model.Model returning a single float or nothing are differentiated. Within the methods, the following is differentiated: assignments to float64 (including parallel assignments if all values are of type float64); returns of float64; standalone calls to methods on the type implementing model.Model (apparently called for side effects on the model). Imported package name ad is reserved. Non-dummy identifiers starting with the prefix for generated identifiers (_ by default) are reserved. Derivatives do not propagate through a function that is not an elemental or a call to a model method. If a derivative is not registered for an elemental, calling the elemental in a differentiated context will cause a run-time error.\nElemental model If method Gradient() []float64 is provided for a model type, the model is treated as an \u0026rsquo;elemental\u0026rsquo; model, and the gradient returned by Gradient() is used by inference algorithms. This allows to code the gradient by hand instead of relying on automatic differentiation.\nElementals Functions are considered elementals (and must have a registered derivative) if their signature is either of kind\nfunc (float64, float64*) float64 that is, one or more non-variadic float64 arguments and float64 return value, or func ([]float64) float64\nFor example, functions\nfunc foo(float64, float64, float64) float64 func bar([]float64) float64 are considered elementals, while functions\nfunc fee(...float64) float64 func buz(int, float64) float64 are not. Gradients for selected functions from the math package are pre-defined (Sqrt, Exp, Log, Pow, Sin, Cos, Tan, Erf, Erfc). Auxiliary elemental functions with pre-defined gradients are in bitbucket.org/dtolpin/infergo/mathx.\nDistributions Distributions are models. Several distributions are provided in bitbucket.org/dtolpin/infergo/dist. In addition to the Observe method, distributions have Logp (single observation) and Logps (multiple observations) methods which accept distribution parameters and observations as individual arguments rather than in a single slice.\nDifferentiation Command-line utility deriv is used to differentiate a model. The command-line syntax is:\nderiv path/to/model/package For example,\nderiv examples/hello/model Run deriv -h for the full list of command-line options. The differentiated model is put into subpackage \u0026ldquo;ad\u0026rdquo; of the model\u0026rsquo;s package, with the same name as the original package.\nInference For inference, infergo offers\noptimization via gradient ascent methods. full posterior inference via Hamiltonian Monte Carlo variants. Optimization An optimizer implements interface infer.Grad. Interface implementations are gradient ascent with momentum and Adam. Both methods are capable to work with stochastic data (e.g. streams or batches).\nFull posterior An MCMC sampler for full posterior inference implements interface infer.MCMC. Inteface implementations are HMC and NUTS.\nDepthAdapter enables adaption of NUTS step size with respect to the average tree depth.\n","date":"1 January, 0001","id":1,"permalink":"/guide/","summary":"\u003cp\u003e\u003ca href=\"https://godoc.org/bitbucket.org/dtolpin/infergo\"\u003e\u003cimg src=\"https://godoc.org/bitbucket.org/dtolpin/infergo?status.svg\" alt=\"GoDoc\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"code\"\u003eCode\u003c/h2\u003e\n\u003ch3 id=\"model\"\u003eModel\u003c/h3\u003e\n\u003cp\u003eA model is defined in it\u0026rsquo;s own package. The model must implement\ninterface\n\u003ca href=\"https://godoc.org/bitbucket.org/dtolpin/infergo/model#Model\"\u003e\u003ccode\u003emodel.Model\u003c/code\u003e\u003c/a\u003e,\nconsisting of a single method \u003ccode\u003eObserve([]float64) float64\u003c/code\u003e. In\nthe model\u0026rsquo;s source code:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eMethods on the type implementing \u003ccode\u003emodel.Model\u003c/code\u003e returning a\nsingle float or nothing are differentiated.\u003c/li\u003e\n\u003cli\u003eWithin the methods, the following is differentiated:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eassignments to \u003ccode\u003efloat64\u003c/code\u003e (including parallel\nassignments if all values are of type \u003ccode\u003efloat64\u003c/code\u003e);\u003c/li\u003e\n\u003cli\u003ereturns of float64;\u003c/li\u003e\n\u003cli\u003estandalone calls to methods on the type implementing\nmodel.Model (apparently called for side  effects on\nthe model).\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eImported package name \u003ccode\u003ead\u003c/code\u003e is reserved.\u003c/li\u003e\n\u003cli\u003eNon-dummy identifiers starting with the prefix for\ngenerated identifiers (\u003ccode\u003e_\u003c/code\u003e by default) are reserved.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eDerivatives do not propagate through a function that is not\nan elemental or a call to a model method. If a derivative is\nnot registered for an elemental, calling the elemental in a\ndifferentiated context will cause a run-time error.\u003c/p\u003e","tags":"","title":"Guide"},{"content":"infergo comes bundled with basic examples. An example is a good starting point for a new infergo project.\nAdvanced examples and case studies go into a separate repository, https://bitbucket.org/dtolpin/infergo-studies.\n","date":"1 January, 0001","id":2,"permalink":"/examples/","summary":"\u003cp\u003e\u003ccode\u003einfergo\u003c/code\u003e comes bundled with \u003ca href=\"https://bitbucket.org/dtolpin/infergo/src/master/examples/\"\u003ebasic\nexamples\u003c/a\u003e.\nAn example is a good starting point for a new \u003ccode\u003einfergo\u003c/code\u003e\nproject.\u003c/p\u003e\n\u003cp\u003eAdvanced examples and case studies go into a separate\nrepository,\n\u003ca href=\"https://bitbucket.org/dtolpin/infergo-studies\"\u003ehttps://bitbucket.org/dtolpin/infergo-studies\u003c/a\u003e.\u003c/p\u003e","tags":"","title":"Examples"},{"content":"Contributors:\nGehenna Gopher David Tolpin ","date":"1 January, 0001","id":3,"permalink":"/team/","summary":"\u003cp\u003eContributors:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/images/infergopher.png\"\u003eGehenna Gopher\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://offtopia.net/\"\u003eDavid Tolpin\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","tags":"","title":"Team"},{"content":"Infergo v1.2.2 is out.\nInfergo has been made to work with Go 1.25. Accompanying repositories (infergo-studies, gogp) have been updated to depend on this version.\n","date":"7 November, 2025","id":4,"permalink":"/news/v1.2.2/","summary":"\u003cp\u003eInfergo v1.2.2 is \u003ca href=\"https://bitbucket.org/dtolpin/infergo/src/v1.2.2/\"\u003eout\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eInfergo has been made to work with Go 1.25. Accompanying repositories (infergo-studies, gogp) have been updated to depend on this version.\u003c/p\u003e","tags":"","title":"infergo v1.2.2"},{"content":"GoGP v1.0.1 is out. This is the first stable (v1) release of GoGP, a library for Gaussian process regression. GoGP has been used in production for over a year, and has undergone many changes improving performance and robustness.\n","date":"11 July, 2021","id":5,"permalink":"/news/gogp-v1.0.1/","summary":"\u003cp\u003e\u003ca href=\"/peers/gogp\"\u003eGoGP v1.0.1\u003c/a\u003e is\n\u003ca href=\"http://bitbucket.org/dtolpin/gogp/srv/v1.0.1\"\u003eout\u003c/a\u003e. This is the\nfirst stable (v1) release of GoGP, a library for Gaussian\nprocess regression. GoGP has been used in production for over a\nyear, and has undergone many changes improving performance and\nrobustness.\u003c/p\u003e","tags":"","title":"gogp v1.0.1"},{"content":"Infergo v1.0.1 is out.\nThis is the first stable (v1) release of Infergo. Infergo has undergone many changes during the past year, and has been used in production for mission-critical computations in the cloud.\n","date":"11 July, 2021","id":6,"permalink":"/news/v1.0.1/","summary":"\u003cp\u003eInfergo v1.0.1 is \u003ca href=\"https://bitbucket.org/dtolpin/infergo/src/v1.0.1/\"\u003eout\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis is the first stable (v1) release of Infergo. Infergo has\nundergone many changes during the past year, and has been\nused in production for mission-critical computations in the\ncloud.\u003c/p\u003e","tags":"","title":"infergo v1.0.1"},{"content":"GoGP is out. GoGP is a library for Gaussian process regression in Go and uses Infergo for automatic differentiation and inference.\n","date":"24 January, 2020","id":7,"permalink":"/news/gogp/","summary":"\u003cp\u003e\u003ca href=\"/peers/gogp\"\u003eGoGP\u003c/a\u003e is \u003ca href=\"http://bitbucket.org/dtolpin/gogp\"\u003eout\u003c/a\u003e.\nGoGP is a library for Gaussian process regression in Go and uses\nInfergo for automatic differentiation and inference.\u003c/p\u003e","tags":"","title":"gogp v0.1.0"},{"content":"Infergo v0.7.0 is out.\nThis release is a result of improving and extending Infergo along with development of GoGP, a library for Gaussian process regression.\nWhat\u0026rsquo;s new:\nmodel\u0026rsquo;s gradient can be explicitly specified as the Gradient() method, instead of through automatic differentation. An elemental may also be a function which accepts a slice of floats (in addition to functions which accept one or more float scalars as parameters). More kernels in the supplied kernel library. As usual, fixes and performance improvements. ","date":"24 January, 2020","id":8,"permalink":"/news/v0.7.0/","summary":"\u003cp\u003eInfergo v0.7.0 is \u003ca href=\"https://bitbucket.org/dtolpin/infergo/src/v0.7.0/\"\u003eout\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis release is a result of improving and extending Infergo\nalong with development of \u003ca href=\"/peers/gogp\"\u003eGoGP\u003c/a\u003e,\na library for Gaussian process regression.\u003c/p\u003e\n\u003cp\u003eWhat\u0026rsquo;s new:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emodel\u0026rsquo;s gradient can be explicitly specified as the\n\u003ccode\u003eGradient()\u003c/code\u003e method, instead of through automatic\ndifferentation.\u003c/li\u003e\n\u003cli\u003eAn elemental may also be a function which accepts a slice of\nfloats (in addition to functions which accept one or more\nfloat scalars as parameters).\u003c/li\u003e\n\u003cli\u003eMore kernels in the supplied kernel library.\u003c/li\u003e\n\u003cli\u003eAs usual, fixes and performance improvements.\u003c/li\u003e\n\u003c/ul\u003e","tags":"","title":"infergo v0.7.0"},{"content":"Infergo v0.6.1 is out.\nWhat\u0026rsquo;s new:\nI moved many things around, some in a backward-incompatible way, but should only affect a minority of users. As a side-effect of using Infergo for a rather involved model, I fixed two bugs in the automatic differentiation transformation. The bugs manifested in edge cases I didn\u0026rsquo;t even think they exist. The accompanying repository infergo-studies now contains a new case study \u0026mdash; a rewrite of Stan\u0026rsquo;s LDA example. ","date":"25 April, 2019","id":9,"permalink":"/news/v0.6.1/","summary":"\u003cp\u003eInfergo v0.6.1 is \u003ca href=\"https://bitbucket.org/dtolpin/infergo/src/v0.6.1/\"\u003eout\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWhat\u0026rsquo;s new:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI moved many things around, some in a backward-incompatible way, but should\nonly affect a minority of users.\u003c/li\u003e\n\u003cli\u003eAs a side-effect of using Infergo for a rather involved model, I fixed two\nbugs in the automatic differentiation transformation. The bugs manifested\nin edge cases I didn\u0026rsquo;t even think they exist.\u003c/li\u003e\n\u003cli\u003eThe accompanying repository\n\u003ca href=\"https://bitbucket.org/dtolpin/infergo-studies\"\u003einfergo-studies\u003c/a\u003e\nnow contains a new case study \u0026mdash; a rewrite of Stan\u0026rsquo;s LDA\nexample.\u003c/li\u003e\n\u003c/ul\u003e","tags":"","title":"infergo v0.6.1"},{"content":"infergo v0.5.0 is out.\nWhat\u0026rsquo;s new:\nMultithreading support. Differentiation can be performed concurrently in multiple goroutines without locking of calls to Observe or Gradient, and with little contention. Examples and case studies performing inference in parallel, both using Infergo\u0026rsquo;s own inference algorithms, or through integration in Gonum. ","date":"1 April, 2019","id":10,"permalink":"/news/v0.5.0/","summary":"\u003cp\u003e\u003ccode\u003einfergo v0.5.0\u003c/code\u003e is \u003ca href=\"https://bitbucket.org/dtolpin/infergo/src/v0.5.0/\"\u003eout\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWhat\u0026rsquo;s new:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMultithreading support. Differentiation can be performed concurrently in\nmultiple goroutines without locking of calls to Observe or Gradient,\nand with little contention.\u003c/li\u003e\n\u003cli\u003eExamples and case studies performing inference in parallel, both using\nInfergo\u0026rsquo;s own inference algorithms, or through integration in Gonum.\u003c/li\u003e\n\u003c/ul\u003e","tags":"","title":"infergo v0.5.0"},{"content":" 2.16 And the LORD God commanded the man, saying: \u0026lsquo;Of every tree of the garden thou mayest freely eat;\n2.17 but of the tree of the knowledge of good and evil, thou shalt not eat of it; for in the day that thou eatest thereof thou shalt surely die.\u0026rsquo;\nThe Book of Genesis\nGo gives the programmer introspection into every aspect of the language, and of a running program. But to one thing the programmer does not have access, and it is the goroutine identifier. Because the day the programmers know the goroutine identifier, they create goroutine-local storage through shared access and mutexes, and shall surely die.\nHowever, there are use cases beyond concurrent handling of HTTP requests, in which sharing memory by communicating through channels or passing the context around is not going to work. One such case is Infergo. Infergo transforms Go source code to enable reverse-mode automatic differentiation. Function signatures stay unchanged, but function bodies are modified to write to a so-called tape a trace of every floating point operation. A single tape must be accessible by all functions. If derivatives are computed concurrently in multiple goroutines, every goroutine must have its own tape.\nThe functions must know how to get to the tape. And getting to the tape must be very efficient: every floating point operation involves an access to the tape!\nIt is not that no one thought about goroutine identifiers before. There are Go programmers who need the identifiers, some of them admit the need, and a few find workarounds to actually obtain the identifiers. I searched for the workarounds, I found several worthy attempts, and then I had a revelation, and then I discovered someone who had the same revelation before me. And that gave Infergo efficient goroutine-local storage for concurrent automatic differentiation and inference. Here is how it went.\nWorthy Attempts From the makers of Go Brad Fitzpatrick is a member of the Go programming language team at Google. Brad wrote a function which obtains the goroutine identifier. The function creates a stack trace and parses the identifier out of string representation of the trace. Brad needed this for debugging, \u0026ldquo;to track that functions run on the goroutine that they\u0026rsquo;re supposed to\u0026rdquo;. The function uses public API calls and an undocumented but stable format of the serialized stack trace.\nSomewhat cumbersome but working. Unfortunately, too inefficient for Infergo use case. Collecting, serializing, and parsing the stack trace on every operation makes automatic differentiation 2,000 (two thousand) times slower!\nFrom the users of Go JT Olio wrote a goroutine-local storage library. The library \u0026ldquo;defines 16 special functions and embed base-16 tags into the stack using the call order of those 16 functions.\u0026rdquo; Then, this embedding is used to produce an unique goroutine identifier and establish goroutine-local storage. The idea blew me away! However, the library requires that Go routines are created through a library call. I could modify Infergo\u0026rsquo;s own inference algorithms, however I would not be able to pass functions and gradients to third-party code. Infergo integrates with Gonum optimization nicely, and by enabling goroutine-local tapes I strived to improve this integration, rather than sacrifice it.\nRevelation I was almost ready to give up, that is, to write code that adds an extra \u0026lsquo;context\u0026rsquo; parameter to every differentiated function. But then it came down onto me that maybe Go does not want to prevent me from using the goroutine identifier. Maybe it is there, and I just do not see it.\nIndeed, Go has an assembly language. The language is documented, Go functions can be implemented in Go assembly. If I wanted a system feature not available through a library, I would write an assembly function bringing that feature to me.\nThe same goes for Go.\nNot only Go has an assembler, the assembler has a dedicated register g pointing at runtime.g, the goroutine descriptor. goid, the Go routine identifier is just one of the fields of the descriptor. I can just use the contents of g to get the goroutine identifier, and it will only be a couple instructions!\nIt is much easier to find something if you know what you are looking for. Tao Wen wrote yet another GLS library; and this library does exactly what I just described: uses Go assembler to access register g, and retrieves field goid from the structure pointed to by the register. I somewhat simplified the code, added support for all platforms where Go is available, and now Infergo has fast and straightforward support for multithreading.\nLessons Learned It is sometimes easier to find a hole in the fence than to jump over.\nYou can do anything with Go. You just must prove (to yourself more than to others) that you are brave and skilled enough. For example, by diving into Go internals and coding in Go assembly.\nMy Samsung Tab S4 tablet is amazingly well fit for multithreading, in Go in particular. I did most of the development on the tablet, in Termux. The tablet\u0026rsquo;s CPU has 8 cores, and Go runs multiple goroutines in parallel with very little overhead: 8 inference threads in parallel take roughly the same time as a single thread with local goroutine storage, and only 20% slower than a single thread with a global tape, for the same amount of computation per thread.\nYou can run multiple goroutines in parallel in a browser via WebAssembly. WebAssembly is slower than other targets, but still quite fast.\n","date":"1 April, 2019","id":11,"permalink":"/news/tale-of-goids/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e2.16\u003c/strong\u003e And the LORD God commanded the man, saying: \u0026lsquo;Of every tree of the garden\nthou mayest freely eat;\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e2.17\u003c/strong\u003e but of the tree of the knowledge of good and evil,\nthou shalt not eat of it; for in the day that thou eatest thereof thou shalt\nsurely die.\u0026rsquo;\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThe Book of Genesis\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003ca href=\"http://golang.org/\"\u003eGo\u003c/a\u003e gives the programmer introspection into every aspect\nof \u003ca href=\"https://godoc.org/reflect\"\u003ethe language\u003c/a\u003e, and of a \u003ca href=\"https://godoc.org/runtime\"\u003erunning\nprogram\u003c/a\u003e. But to one thing the programmer does not\nhave access, and it is the goroutine identifier. Because the day the\nprogrammers know the goroutine identifier, they create goroutine-local storage\nthrough shared access and mutexes, and shall surely die.\u003c/p\u003e","tags":"","title":"The Tale of GoIDs"},{"content":"infergo v0.3.0 is out.\nWhat\u0026rsquo;s new:\nOnly methods returning float64 or nothing are differentiated. This allows to define helper methods on the model, such as returning the number of parameters, and call the methods outside of differentiated context. infergo models can be optimized using Gonum optimization algorithms. This includes BFGS and variants. Case study lr-gonum applies L-BFGS to linear regression. Case studies have been extended. New studies include: linear regression, solved using either stochastic gradient descent and BFGS; compilation of infergo models and inference into WebAssembly and running in the browser; integration with Gonum; Neal\u0026rsquo;s funnel, a re-parameterization example borrowed from Stan documentation. ","date":"20 March, 2019","id":12,"permalink":"/news/v0.3.0/","summary":"\u003cp\u003e\u003ccode\u003einfergo v0.3.0\u003c/code\u003e is \u003ca href=\"https://bitbucket.org/dtolpin/infergo/src/v0.3.0/\"\u003eout\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWhat\u0026rsquo;s new:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOnly methods returning float64 or nothing are differentiated. This allows\nto define helper methods on the model, such as returning the number of\nparameters, and call the methods outside of differentiated context.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003einfergo\u003c/code\u003e models can be optimized using \u003ca href=\"https://godoc.org/gonum.org/v1/gonum/optimize\"\u003eGonum optimization\nalgorithms\u003c/a\u003e. This includes\nBFGS and variants. Case study\n\u003ca href=\"https://bitbucket.org/dtolpin/infergo-studies/src/master/lr-gonum/\"\u003elr-gonum\u003c/a\u003e\napplies L-BFGS to linear regression.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bitbucket.org/dtolpin/infergo-studies\"\u003eCase studies\u003c/a\u003e have been extended. New\nstudies include:\n\u003cul\u003e\n\u003cli\u003elinear regression, solved using either stochastic gradient descent and BFGS;\u003c/li\u003e\n\u003cli\u003ecompilation of \u003ccode\u003einfergo\u003c/code\u003e models and inference into WebAssembly and running in\nthe browser;\u003c/li\u003e\n\u003cli\u003eintegration with Gonum;\u003c/li\u003e\n\u003cli\u003eNeal\u0026rsquo;s funnel, a re-parameterization example borrowed from \u003ca href=\"https://mc-stan.org/users/documentation/\"\u003eStan documentation\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","tags":"","title":"infergo v0.3.0"},{"content":"infergo v0.2.2 is out.\nWhat\u0026rsquo;s new:\nConstant folding. Automatic import of packages required for short variable declarations (see issue #10). ","date":"9 December, 2018","id":13,"permalink":"/news/v0.2.2/","summary":"\u003cp\u003e\u003ccode\u003einfergo v0.2.2\u003c/code\u003e is \u003ca href=\"https://bitbucket.org/dtolpin/infergo/src/v0.2.2/\"\u003eout\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWhat\u0026rsquo;s new:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConstant folding.\u003c/li\u003e\n\u003cli\u003eAutomatic import of packages required for short variable declarations\n(see \u003ca href=\"https://bitbucket.org/dtolpin/infergo/issues/10\"\u003eissue #10\u003c/a\u003e).\u003c/li\u003e\n\u003c/ul\u003e","tags":"","title":"infergo v0.2.2"},{"content":"infergo v0.2.1 is out.\nWhat\u0026rsquo;s new:\ninfergo.org ","date":"27 November, 2018","id":14,"permalink":"/news/v0.2.1/","summary":"\u003cp\u003e\u003ccode\u003einfergo v0.2.1\u003c/code\u003e is \u003ca href=\"https://bitbucket.org/dtolpin/infergo/src/v0.2.1/\"\u003eout\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWhat\u0026rsquo;s new:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://infergo.org\"\u003einfergo.org\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","tags":"","title":"infergo v0.2.1"},{"content":" I share here my experiences from integrating probabilistic programming into a server-side software system and implementing a probabilistic programming facility for Go, a modern programming language of choice for server-side software development. Server-side application of probabilistic programming poses challenges for a probabilistic programming system. I discuss the challenges and my experience in overcoming them, and suggest guidelines that can help in a wider adoption of probabilistic programming in server-side software systems.\nChallenges of Server-Side Probabilistic Programming Incorporating a probabilistic program, or rather a probabilistic procedure, within a larger code body appears to be rather straightforward: one implements the model in the probabilistic programming language, fetches and preprocesses the data in the host programming language, passes the data and the model to an inference algorithm, and post-processes the results in the host programming language again to make algorithmic decisions based on inference outcomes. However, complex server-side software systems make integration of probabilistic inference challenging.\nSimulation vs. inference Probabilistic models often follow a design pattern of simulation-inference: a significant part of the model is a simulator, running an algorithm with fixed parameters; the optimal parameters, or their distribution, are to be inferred. The inferred parameters are then used by the software system to execute the simulation independently of inference for forecasting and decision making.\nThis pattern suggests re-use of the simulator: instead of implementing the simulator twice, in the probabilistic model and in the host environment, the same code can serve both purposes. However to achieve this, the host language must coincide with the implementation language of the probabilistic model, on one hand, and allow a computationally efficient implementation of the simulation, on the other hand. Some probabilistic systems (Figaro, Anglican, Turing) are built with tight integration with the host environment in mind; more often than not though the probabilistic code is not trivial to re-use.\nData interface In a server-side application data for inference comes from a variety of sources: network, databases, distributed file systems, and in many different formats. Efficient inference depends on fast data access and updating. Libraries for data access and manipulation are available in the host environment. While the host environment can be used as a proxy retrieving and transforming the data, such as in the case of Stan integrations, sometimes direct access from the probabilistic code is the preferred option, for example when the data is streamed or retrieved conditionally.\nIntegration and deployment Deployment of server-side software systems is a delicate process involving automatic builds and maintenance of dependencies. Adding a component, which possibly introduces additional software dependencies or even a separate runtime, complicates deployment. Minimizing the burden of probabilistic programming on the integration and deployment process should be a major consideration in design or selection of probabilistic programming tools. Probabilistic programming systems that are implemented or provide an interface in a popular programming language, e.g. Python (Edward, Pyro) are easier to integrate and deploy, however the smaller the footprint of a probabilistic system, the easier is the adoption.\nProbabilistic Programming Facility for Go Based on the experience of developing and deploying solutions using different probabilistic environments, I propose guidelines to implementation of a probabilistic programming facility for server-side applications. I believe that these guidelines, when followed, help easier integration of probabilistic programming inference into large-scale server-side software systems.\nA probabilistic model should be programmed in the host programming language. The facility may impose a discipline on model implementation, such as through interface constraints, but otherwise supporting unrestricted use of the host language for implementation of the model.\nBuilt-in and user-defined data structures and libraries should be accessible in the probabilistic programming model. Inference techniques relying on the code structure, such as those based on automatic differentiation, should support the use of common data structures of the host language.\nThe model code should be reusable between inference and simulation. The code which is not required solely for inference should be written once for both inference of parameters and use of the parameters in the host environment. It should be possible to run simulation outside the probabilistic model without runtime or memory overhead imposed by inference needs.\nIn line with the guidelines, I have implemented a probabilistic programming facility for the Go programming language, infergo (http://infergo.org/). I have chosen Go because Go is a small but expressive programming language with efficient implementation, which has recently become quite popular for computation-intensive server-side programming. This facility is already used in production environment for inference of mission-critical algorithm parameters.\nA probabilistic model in infergo is an implementation of the Model interface requiring a single method Observe which accepts a vector (a Go slice) of floats, the parameters to infer, and returns a single float, interpreted as unnormalized log-likelihood of the posterior distribution. Implementation of model methods can be written in virtually unrestricted Go and use any Go libraries.\nFor inference, infergo relies on automatic differentiation. The source code of the model is translated by a command-line tool provided by infergo into an equivalent model with reverse-mode automatic differentiation of the log-likelihood with respect to the parameters applied. The differentiation operates on the built-in floating-point type and incurs only a small computational overhead. However, even this overhead is avoided when the model code is executed outside of inference algorithms: both the original and the differentiated model are simultaneously available to the rest of the program code, so the methods can be called on the differentiated model for inference, and on the original model for the most efficient execution with the inferred parameters.\nThe Go programming language and development environment offer capabilities which made implementation of infergo affordable.\nThe Go parser and abstract syntax tree serializer are a part of the standard library. Parsing, transforming, and generating Go source code is straightforward and effortless. Type inference (or type checking as it is called in the Go ecosystem), also provided in the standard library, augments parsing and allows to selectively apply transformation-based automatic differentiation based on static expression types. Go compiles and runs fast. Fast compilation and execution speeds allow to use the same facility for both exploratory design of probabilistic models and for inference in production environment. Go offers efficient parallel execution as a first-class feature, via so-called goroutines. Goroutines streamline implementation of sampling-based inference algorithms. Sample generators and consumers are run in parallel, communicating through channels. Inference is easy to parallelize in order to exploit hardware multi-processing, and samples are retrieved lazily for postprocessing. Table 1 provides memory consumption and running time measurements on basic models to illustrate infergo\u0026rsquo;s performance. The measurements were obtained on a 2.3GHz Intel Core 5 CPU with 8GB of memory for 1000 iterations of Hamiltonian Monte Carlo with 10 leapfrog steps. Note that log-likelihood computation for standard distributions is not optimized yet. Quite the opposite: since models in infergo are fully composable, primitive distributions are themselves implemented as infergo models and automatically differentiated.\nTable 1: Memory and running times for 1000 iterations of HMC with 10 leapfrog steps.\nmodel compilation time execution time memory 8 schools 0.15s 0.6s 5.5MB 10D normal, 100 points 0.15s 2.0s 5.7MB 50D normal, 100 points 0.15s 9.0s 5.8MB A lightweight probabilistic programming facility similar to infergo can be added to most modern general-purpose programming languages, in particular those used in implementing large-scale software systems, making probabilistic programming inference more accessible in server-side applications.\n","date":"26 November, 2018","id":15,"permalink":"/news/why/","summary":"\u003cblockquote\u003e\n\u003cp\u003eI share here my experiences from integrating probabilistic\nprogramming into a server-side software system and\nimplementing a  probabilistic programming facility for Go, a\nmodern programming language of choice for server-side software\ndevelopment.  Server-side application of probabilistic\nprogramming poses challenges for a probabilistic programming\nsystem. I discuss the challenges and my experience in\novercoming them, and suggest guidelines that can help in a\nwider adoption of probabilistic programming in server-side\nsoftware systems.\u003c/p\u003e","tags":"","title":"Why infergo"},{"content":"GoGP is a library for probabilistic programming around Gaussian processes. It uses Infergo for automatic differentiation and inference.\nGoGP is built around a dual view on the Gaussian process\nas a stochastic process, as a probabilistic model with respect to kernel. Gaussian process instance GP, the Gaussian process type, encapsulates similarity and noise kernels, their parameters, and observation inputs and outputs:\n// Type GP is the barebone implementation of GP. type GP struct { NDim int // number of dimensions Simil, Noise Kernel // kernels ThetaSimil, ThetaNoise []float64 // kernel parameters X [][]float64 // inputs Y []float64 // outputs Parallel bool // when true, covariances are computed in parallel } Public methods defined on GP fall into two groups: Gaussian process fitting and prediction, on one hand, and probabilistic model interface, on the other hand.\nGaussian process methods Absorb updates GP with observations, Produce returns predicted outputs for unseen inputs.\nfunc (*GP) Absorb(x [][]float64, y []float64) func (*GP) Produce(x [][]float64) (mu, sigma []float64) When kernel parameters are fixed (known or learned), Absorb and Produce are all that is needed for posterior Gaussian process inference.\nProbabilistic model methods Observe and Gradient turn a GP instance into an elemental Infergo model (that is, a model with supplied gradient). The model can be passed to inference algorithms or used within another model.\nfunc (*GP) Observe(x []float64) float64 func (*GP) Gradient() []float64 // `elemental\u0026#39; model If the length of x is equal to the number of kernel hyperparameters (Simil.NTheta() + Noise.NTheta()) then the gradient of marginal likelihood is computed with respect to kernel hyperparameters only. Observations must be provided in the fields of the GP instance. Otherwise, if the length of x is greater than the number of parameters, the rest of x is interpreted as observations. In the latter case, the gradient is computed with respect to both kernel hyperparameters and observations.\nHyperparameter priors Type Model is a wrapper model combining a GP instance and priors on hyperparameters into an elemental Infergo model. type Model struct { *GP Priors model.Model } GP holds a Gaussian process instance and Priors holds an instance of the model expressing beliefs about hyperparameters.\nKernels There are two kernel kinds:\nsimilarity kernel; noise kernel. Both kinds must satisfy the Kernel interface:\ntype Kernel interface { Observe([]float64) float64 NTheta() int } The Observe method computes the variance or covariance, the NTheta method returns the number of kernel parameters.\nA similarity (or covariance) kernel receives concanetation of kernel parameters and coordinates of two points. A noise kernel receives concatenation of kernel parameters and coordinates of a single point. Here is an example implementation of the RBF (or normal) kernel:\ntype RBF struct{} func (RBF) Observe(x []float64) float64 { l, xa, xb := x[0], x[1], x[2] d := (xa - xb) / l return math.Exp(-d * d / 2) } func (RBF) NTheta() int { return 1 } Frequently used primitive kernels are included in the library.\nCase studies GoGP includes case studies, illustrating, on simple examples, common patterns of GoGP use. We briefly summarize here some of the case studies.\nBasic usage In the basic case, similar to that supported by many Gaussian process libraries, a GP instance directly serves as the model for inference on hyperparameters (or the hyperparameters can be just fixed).\nThe library user specifies the kernel:\ntype Basic struct{} func (Basic) Observe(x []float64) float64 { return x[0] * kernel.Normal.Observe(x[1:]) } func (Basic) NTheta() int { return 2 } and initializes GP with a kernel instance: gp := \u0026amp;gp.GP{ NDim: 1, Simil: Basic{}, }\nMLE inference on hyperparameters and prediction can then be performed through library functions.\nPriors on hyperparameters If priors on hyperparameters are to be specified, the library user provides both the kernel and the prior beliefs: // Similarity kernel type Simil struct{} func (Simil) Observe(x []float64) float64 { const ( c = iota // trend scale l // trend length scale xa // first point xb // second point ) return x[c]*kernel.Matern52.Cov(x[l], x[xa], x[xb]) } func (Simil) NTheta() int { return 2 } // Noise kernel type Noise struct{} func (Noise) Observe(x []float64) float64 { return 0.01 * kernel.UniformNoise.Observe(x) } func (Noise) NTheta() int { return 1 } // Hyperparameter priors type Priors struct{} func (*Priors) Observe(x []float64) float64 { return Normal.Logps(1, 1, x...) } A Model instance is then used to combine them: m := \u0026amp;gp.Model{ GP: \u0026amp;gp.GP{ NDim: 1, Simil: Simil, Noise: Noise, }, Priors: \u0026amp;Priors{}, } Maximum a posteriori hyperparameter assignments are affected by both the data and the priors.\nUncertain observation inputs When observation inputs are uncertain, beliefs about inputs can be specified, and the log-likelihood gradient can be computed with respect to both hyperparameters and observation inputs. For example, in a time series, one can assume that observation inputs come from a renewal process and let the inputs move relative to each other. Then, forecasting can be performed relative to posterior observation inputs.\nNon-Gaussian noise In basic usage, the observation noise is assumed to be Gaussian. This usage is supported by initializing GP with a noise kernel, along with a similarity kernel. When the noise is not Gaussian, an analytical solution for posterior Gaussian process inference does not always exist. However, non-Gaussian noise is straightforwardly supported by GoGP through supplying a model for beliefs on observation outputs: type Noise struct { Y []float64 // noisy outputs } func (m *Noise) Observe(x []float64) (lp float64){ // Laplacian noise for i := range m.Y { lp += Expon.Logp(1/math.Exp(x[0]), math.Abs(m.Y[i]-x[1+i])) } return lp }\n","date":"1 January, 0001","id":16,"permalink":"/peers/gogp/","summary":"\u003cp\u003e\u003ca href=\"http://bitbucket.org/dtolpin/gogp\"\u003eGoGP\u003c/a\u003e is a\nlibrary for probabilistic programming around Gaussian processes.\nIt uses \u003c!-- raw HTML omitted --\u003eInfergo\u003c!-- raw HTML omitted --\u003e for automatic\ndifferentiation and inference.\u003c/p\u003e\n\u003cp\u003eGoGP is built around a dual view on the Gaussian process\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eas a stochastic process,\u003c/li\u003e\n\u003cli\u003eas a probabilistic model with respect to kernel.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"gaussian-process-instance\"\u003eGaussian process instance\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://pkg.go.dev/bitbucket.org/dtolpin/gogp/gp#GP\"\u003e\u003ccode\u003eGP\u003c/code\u003e\u003c/a\u003e, the Gaussian process type, encapsulates similarity and\nnoise kernels, their parameters, and observation inputs and\noutputs:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#60a0b0;font-style:italic\"\u003e// Type GP is the barebone implementation of GP.\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#bbb\"\u003e\u003c/span\u003e\u003cspan style=\"color:#007020;font-weight:bold\"\u003etype\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e \u003c/span\u003eGP\u003cspan style=\"color:#bbb\"\u003e \u003c/span\u003e\u003cspan style=\"color:#007020;font-weight:bold\"\u003estruct\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e \u003c/span\u003e{\u003cspan style=\"color:#bbb\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#bbb\"\u003e\t\u003c/span\u003eNDim\u003cspan style=\"color:#bbb\"\u003e                   \u003c/span\u003e\u003cspan style=\"color:#902000\"\u003eint\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e       \u003c/span\u003e\u003cspan style=\"color:#60a0b0;font-style:italic\"\u003e// number of dimensions\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#bbb\"\u003e\t\u003c/span\u003eSimil,\u003cspan style=\"color:#bbb\"\u003e \u003c/span\u003eNoise\u003cspan style=\"color:#bbb\"\u003e           \u003c/span\u003eKernel\u003cspan style=\"color:#bbb\"\u003e    \u003c/span\u003e\u003cspan style=\"color:#60a0b0;font-style:italic\"\u003e// kernels\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#bbb\"\u003e\t\u003c/span\u003eThetaSimil,\u003cspan style=\"color:#bbb\"\u003e \u003c/span\u003eThetaNoise\u003cspan style=\"color:#bbb\"\u003e \u003c/span\u003e[]\u003cspan style=\"color:#902000\"\u003efloat64\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e \u003c/span\u003e\u003cspan style=\"color:#60a0b0;font-style:italic\"\u003e// kernel parameters\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#bbb\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#bbb\"\u003e\t\u003c/span\u003eX\u003cspan style=\"color:#bbb\"\u003e \u003c/span\u003e[][]\u003cspan style=\"color:#902000\"\u003efloat64\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e \u003c/span\u003e\u003cspan style=\"color:#60a0b0;font-style:italic\"\u003e// inputs\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#bbb\"\u003e\t\u003c/span\u003eY\u003cspan style=\"color:#bbb\"\u003e \u003c/span\u003e[]\u003cspan style=\"color:#902000\"\u003efloat64\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e   \u003c/span\u003e\u003cspan style=\"color:#60a0b0;font-style:italic\"\u003e// outputs\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#bbb\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#bbb\"\u003e\t\u003c/span\u003eParallel\u003cspan style=\"color:#bbb\"\u003e \u003c/span\u003e\u003cspan style=\"color:#902000\"\u003ebool\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e \u003c/span\u003e\u003cspan style=\"color:#60a0b0;font-style:italic\"\u003e// when true, covariances are computed in parallel\u003c/span\u003e\u003cspan style=\"color:#bbb\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#bbb\"\u003e\u003c/span\u003e}\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ePublic methods defined on \u003ccode\u003eGP\u003c/code\u003e fall into two groups: Gaussian\nprocess fitting and prediction, on one hand, and\nprobabilistic model interface, on the other hand.\u003c/p\u003e","tags":"","title":"GoGP"}]